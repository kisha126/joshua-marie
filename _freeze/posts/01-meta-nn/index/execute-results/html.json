{
  "hash": "042404b92b29ecbbfcaca998340407c6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression\"\nsubtitle: \"Use case of code generation in practice\"\nauthor: \"Joshua Marie Ongcoy\"\ndate: \"2025-09-23\"\ncategories: [R, machine-learning, torch, pointless-code]\nformat:\n  html:\n    toc: true\n    toc-float: true\n    toc-depth: 3\n    number-sections: true\n    code-fold: false\n    code-tools: false\n    theme: default\n    highlight-style: tango\n    fig-width: 10\n    fig-height: 6\n    fig-cap-location: bottom\n    df-print: paged\nexecute:\n  echo: true\n  message: false\n  warning: false\n---\n\n\n# Generating torch DFFNN expression\n\nI have a course tutorial, which I discuss the things to get \"advance\" in R. Code generation is part of it. My blog compiles *pointless codes* in *pointless code series*, and this is one of them. \n\nNow, the question is: How do you define neural network architectures in your deep learning projects? Manually write out each layer? Copy-paste and modify existing code? In this part, I wanted to discuss it to you on how to leverage code generation technique that generates 'torch' neural network modules in a *programmatic* approach. This is a handy approach to building flexible, reusable neural network architectures without repetitive code.\n\nI'll walk through a function that creates DFFNN expressions with customizable architectures, layer by layer, explaining each step along the way.\n\n---\n\n## Introduction\n\nThe `create_nn_module()` function dynamically generates **torch neural network module definitions**. Instead of manually writing out layer definitions and forward pass logic, this function builds the code expressions for you.\n\n**Key benefits:**\n\n- **Flexibility**: Change network architecture with a single function call\n- **Automation**: Generate multiple network configurations programmatically\n- **Experimentation**: Quickly test different architectures in hyperparameter searches\n\nThis is how it's done:\n\n1. Define the network architecture (input size, hidden layers, output size)\n2. Specify activation functions for each layer\n3. Programmatically generate the `initialize` method (layer definitions)\n4. Programmatically generate the `forward` method (forward pass logic)\n5. Return an `nn_module` expression ready to be evaluated\n\nThe packages used:\n\n1. [rlang (v1.1.4)](https://rlang.r-lib.org/) - For metaprogramming tools\n2. [purrr (v1.0.2)](https://purrr.tidyverse.org/) - For functional programming\n3. [glue (v1.7.0)](https://glue.tidyverse.org/) - For string interpolation\n4. [magrittr](https://magrittr.tidyverse.org/) - For pipe operator\n5. [box (v1.2.0)](https://github.com/klmr/box) - For modular code organization\n\n## The Complete Function\n\nI created `create_nn_module()` function a while ago and shared it on [GitHub Gist](https://gist.github.com/kisha126/c19c209fbe5e0532e16520649bf142c2). Here's the function we'll be analyzing:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncreate_nn_module = function(nn_name = \"DeepFFN\", hd_neurons = c(20, 30, 20, 15), no_x = 10, no_y = 1, activations = NULL) {\n    box::use(\n        rlang[new_function, call2, caller_env, expr, exprs, sym, is_function, env_get_list],\n        purrr[map, map2, reduce, set_names, compact, map_if, keep, map_lgl], \n        glue[glue], \n        magrittr[`%>%`]\n    )\n    \n    nodes = c(no_x, hd_neurons, no_y)\n    n_layers = length(nodes) - 1\n    \n    call_args = match.call()\n    activation_arg = call_args$activations\n    \n    if (is.null(activations)) {\n        activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n    } else if (length(activations) == 1 || is.function(activations)) {\n        single_activation = activations\n        activations = c(rep(list(single_activation), length(hd_neurons)), list(NA))\n    }\n    \n    activations = map2(activations, seq_along(activations), function(x, i) {\n        if (is.null(x)) {\n            NULL\n        } else if (is.function(x)) {\n            if(!is.null(activation_arg) && is.call(activation_arg) && activation_arg[[1]] == quote(c)) {\n                func_name = as.character(activation_arg[[i + 1]])\n                sym(func_name)\n            } else if(!is.null(activation_arg) && (is.symbol(activation_arg) || is.character(activation_arg))) {\n                func_name = as.character(activation_arg)\n                sym(func_name)\n            } else {\n                parent_env = parent.frame()\n                env_names = ls(envir = parent_env)\n                matching_names = env_names %>%\n                    keep(~ {\n                        obj = env_get_list(parent_env, .x)[[1]]\n                        identical(obj, x)\n                    })\n                \n                if (length(matching_names) > 0) {\n                    sym(matching_names[1])\n                } else {\n                    stop(\"Could not determine function name for activation function\")\n                }\n            }\n        } else if (is.character(x)) {\n            if (length(x) == 1 && is.na(x)) {\n                NULL\n            } else {\n                sym(x)\n            }\n        } else if (is.symbol(x)) {\n            x\n        } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n        }\n    })\n    \n    init_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), call2(\"nn_linear\", !!!dims))\n    })\n    \n    init = new_function(\n        args = list(), \n        body = call2(\"{\", !!!init_body)\n    )\n    \n    layer_calls = map(1:n_layers, function(i) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        activation_fn = if (i <= length(activations)) activations[[i]] else NULL\n        \n        result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n        if (!is.null(activation_fn)) {\n            result = append(result, list(call2(activation_fn)))\n        }\n        result\n    }) |> \n        unlist() |> # recursive = FALSE is also valid\n        compact()\n    \n    forward_body = reduce(layer_calls, function(acc, call) {\n        expr(!!acc %>% !!call)\n    }, .init = expr(x))\n    \n    forward = new_function(\n        args = list(x = expr()), \n        body = call2(\"{\", forward_body)\n    )\n    \n    call2(\"nn_module\", nn_name, initialize = init, forward = forward)\n}\n```\n:::\n\n\n---\n\n### Why box?\n\nYou'll notice that I've been using another approach to load namespace in R. But, why 'box'? \nYou need to check out my mini book dedicated on *modular programming in R*. \n\n### *But why load dependencies using `box::use()` inside a function?*\n\nWell, a function, or a function call, creates an **environment**, which *encloses* the objects and operations within it. In other words, we create a *closure*. This is actually a good practice for several reasons:\n\n1.    **Namespace isolation**: Dependencies loaded inside the function will not make pollution the global environment, or conflicts with any packages loaded. When you load packages required with `library()`, inside a function or not, it attaches those packages to your search path, and will mask functions from other packages. With `box::use()` inside a function, the imports are scoped only to that function's or call's environment. \n\n2.    **Explicit dependencies**: Anyone reading the function immediately knows what external tools it uses. You don't have to scroll to the top of a script to see what's loaded.\n\n3.    **Reproducibility**: The function becomes self-contained. If you share just this function, others know exactly what packages they need less hunting through documentation.\n\n4.    **Avoiding conflicts**: Different functions can use different versions or implementations without conflicts. For example, one function might use `dplyr::filter()` while another uses `stats::filter()`, and they won't interfere with each other.\n\n5.    **Lazy loading**: The packages are only loaded when the function is actually called, not when it's defined. This can improve script startup time if you have many functions but only use a few.\n\n::: callout-note\n\nIn a nutshell: The 'box' package provides explicit, granular imports, making it transparent which namespace to be imported from which packages. It's like having a well-organized toolbox where each tool is labeled.\n\n:::\n\n---\n\n## Explanations\n\nI'll be trying to be concise on explaining each layers of the function so that you'll understand what I did\n\n### Step 1: Loading Dependencies\n\nI use `box::use()` to load dependencies:\n\n-   **rlang**: Improvised Core R programming. One of the core R programming, *metaprogramming* which includes creating expressions and functions programmatically, are less painful than what base R have. \n-   **purrr**: Improvised Functional programming utilities.\n-   **glue**: R lacks Python's f-string for string interpolation, although we have `sprintf()` and `paste()` for that. `glue` makes string interpolation more readable with `glue(\"fc{i}\")` instead of `paste0(\"fc\", i)` or `sprintf(\"fc%d\", i)`.\n-   **magrittr**: The pipe operator `%>%` for chaining operations. This is optional, by the way — R 4.1+ has the native pipe `|>`, but `%>%` offers better flexibility with the dot placeholder. \n\n### Step 2: Defining Network Architecture\n\nIn DFFNN architecture, it is defined by the input layer, the hidden layer, and the output layer. \n\n![Source: https://medium.com/data-science/designing-your-neural-networks-a5e4617027ed](nn-struc.png){fig-align=\"center\"}\n\nThe number of nodes are defined by integers, except for input and output layer nodes which they are *fixed* and determined by the data you provided, and they are defined by `no_x` and `no_y`. \nThe number of hidden layers is defined by the length of input in `hd_neurons` argument. \n\nCombine `no_x`, `hd_neurons`, `no_y` in order: \n\n``` r\nnodes = c(no_x, hd_neurons, no_y)\n```\nAnd then calculate the length of `nodes`, which is $1 + n_{\\text{hidden layres}} + 1$, and then subtract it by 1 because the applied activation functions is invoked *between each layer*.\n\n``` r\nn_layers = length(nodes) - 1\n```\n\n#### Example\n\nWhen you have: \n\n-   10 predictors\n-   5 hidden layers, and for each layer: \n    \n    1.  20 nodes\n    2.  30 nodes\n    3.  20 nodes\n    4.  15 nodes\n    5.  20 nodes\n\n-   1 response variable\n\n*Total number of layers: 7*\n\nThis means we need 7 - 1 linear transformations, and here is my diagram: \n\n$$10_{\\text{inputs}} \\rightarrow20_{\\text{hd1}} \\rightarrow30_{\\text{hd2}} \\rightarrow20_{\\text{hd3}} \\rightarrow15_{\\text{hd4}} \\rightarrow20_{\\text{hd5}} \\rightarrow1_{\\text{ouput}}$$\n\n\n### Step 3: Setting Activation Functions\n\nThe `activations` argument holds the account of the activation function. It could be a string, a literal function, or a mix of it in a vector of inputs. \n\nThen, set `activations = NULL`, where `NULL` is the default value, which leads to set ReLU (`nnf_relu`) as the activation function for all hidden layers\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nif (is.null(activations)) {\n    activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n}\n```\n:::\n\n\nEvery `activations` will have `NA` as the last element because we need to ensure no activation function *after* the output. The output layer often doesn't need an activation (for regression) or needs a specific one based on the task (softmax for classification, sigmoid for binary classification). By defaulting to `NA`, the user can decide.\n\n::: {.callout-note title=\"Length of inputs\"}\n\nTo provide values in `activations` argument, it needs to be equal to the size of hidden layers, or if you provide only 1 act. function, this will be the activation function across the transformations. \n\n:::\n\n::: {.callout-note title=\"Default\"}\n\nThe default is `NULL`. That is, if `activations` is not provided, the activation function is set to ReLU function. \n\n:::\n\n::: {.callout-tip title=\"Instead of NULL\"}\n\nNow, if you're asking \"Why needs to set `activations` to `\"nnf_relu\"` instead of `NULL`\"? Don't worry, I did consider that, but this is just a *pure* demo. \n\n:::\n\n### Step 4: Processing Activation Functions\n\nThis part (re)processes the activation function inputs in the `activations` argument. This kept tracks the argument you are putting, especially when you the input you are writing in `activations` argument has different types. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncall_args = match.call()\nactivation_arg = call_args$activations\n\nactivations = map2(activations, seq_along(activations), function(x, i) {\n    if (is.null(x)) {\n        NULL\n    } else if (is.function(x)) {\n        if(!is.null(activation_arg) && is.call(activation_arg) && \n           activation_arg[[1]] == quote(c)) {\n            func_name = as.character(activation_arg[[i + 1]])\n            sym(func_name)\n        } else {\n            func_name = names(which(sapply(ls(envir = parent.frame()), \n                function(name) {\n                    identical(get(name, envir = parent.frame()), x)\n                })))[1]\n            if (!is.na(func_name)) {\n                sym(func_name)\n            } else {\n                stop(\"Could not determine function name for activation function\")\n            }\n        }\n    } else if (is.character(x)) {\n        if (length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            sym(x)\n        }\n    } else if (is.symbol(x)) {\n        x\n    } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n        NULL\n    } else {\n        stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n    }\n})\n```\n:::\n\n\n### Step 5: Building the `initialize` Method Body\n\nThe *body* I am referring in `initialize` method is the *body* of the *function* for the `initialize` implemented method. This part is a bit far from trivial. I named it `init_body` to keep track the expression I am trying to build. \n\n::: {.callout-note title=\"In reality\"}\n\nKeep in mind that there's no `initialize` and `forward` parameters within `nn_module()` torch namespace or whatsoever. However, it is expected you to create them to create a module inside `nn_module()`. These parameters are kept within the `...` wildcard parameter. \n\n:::\n\n#### Creation of the expressions inside the body\n\nHere is the part I am tracking inside `create_nn_module` body expression: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ninit_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), \n    function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), \n              call2(\"nn_linear\", !!!dims))\n    })\n```\n:::\n\n\nWhat it does is it creates assignment expressions for each layer in the network.\n\nFor instance, `c(20, 30, 20, 15, 20)` is your argument for the `activations`:\n\n1.  `map2(nodes[-length(nodes)], nodes[-1], c)` pairs consecutive layer sizes:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code  code-fold=\"true\"}\n    list(\n        c(10, 20), \n        c(20, 30), \n        c(30, 20), \n        c(20, 15), \n        c(15, 20), \n        c(20, 1)\n    )\n    ```\n    :::\n\n\n2.  For each pair, generates a layer assignment expression:\n\n    -  Layer names: `fc1`, `fc2`, ..., `out` (last layer)\n    -  Creates: `self$fc1 = nn_linear(10, 20)`\n\n*This will be the generated expression:*\n\n``` r\nself$fc1 = nn_linear(10, 20)\nself$fc2 = nn_linear(20, 30)\nself$fc3 = nn_linear(30, 20)\nself$fc4 = nn_linear(20, 15)\nself$fc5 = nn_linear(15, 20)\nself$out = nn_linear(20, 1)\n```\n\n::: {.callout-note title=\"How is it done?\"}\n\nI need you to understand `rlang::call2()` a bit: \n\nThe `call2()` function is a glorified `call()` from base R that builds function call expressions.\n\nFrom what I did within `init_body`: \n\n-  `call2(\"$\", expr(self), sym(\"fc1\"))` constructs `self$fc1`\n-  `call2(\"nn_linear\", !!!dims)` is a bit complex:\n\n    -   It splices `dims` from what I created in `map2(nodes[-length(nodes)], nodes[-1], c)`. \n    -   `call2()` function accepts rlang's quasiquotation API, then splices the dimensions, i.e. `call2(\"nn_linear\", !!!c(10, 20))` to `call2(\"nn_linear\", 10, 20)`. \n    -   Then finally constructs `nn_linear(10, 20)`\n\n-  `call2(\"=\", lhs, rhs)` parses an expression: `lhs = rhs`. This part yields an expression I want: `self$fc1 = nn_linear(10, 20)`. \n\n*Note: You can use `<-` if you want, instead of `=`. After all, `=` within `call2()`'s `.fn` argument tokenize `=` as an assignment operator. *\n\n:::\n\n#### Building an *actual* body and function\n\nNow, for this part: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ninit = new_function(\n    args = list(), \n    body = call2(\"{\", !!!init_body)\n)\n```\n:::\n\n\nDon't forget to put curly brackets `{` around the built expression because it becomes necessary in R when composing a function with multiple lines. Still using `call2()` for that, particularly `call2(\"{\", !!!init_body)` creates a code block `{ ... }` containing all initialization statements. The `!!!` operator \"splices\" the list of expressions into the block, because `init_body` forms a list of expressions.\n\nAfter building the expression I want for the body of `initialize`, let's take further by utilizing it as a *body* to create a function with `rlang::new_function`. I just simply wraps all the layer initialization expressions into a complete function for `initialize` method for `nn_module()`. \n\n::: {.callout-note title=\"Inputs in `initialize`\"}\n\nNotice that the argument for `initialize` is *empty*? I could've place `input_size` and `output_size` if I wanted to, but it seems unnecessary since I already placed the sizes of the input and output within the expression I built. To make a function expression with empty arguments, place the `args` argument of `new_function` with empty `list()`. \n\n:::\n\nHere's the result:\n\n```r\nfunction() {\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}\n```\n\n*Store this expression into `init` because we still have to finalize the expression we want to create. *\n\n### Step 6: Building Layer Calls for Forward Pass\n\nThe same process as `initialize`, except we are not building multiple lines of expression, just building a *chained expression* with 'magrittr' pipe from the initial value.\n\n#### Creating layer of calls\n\nTo form this expression is also complex\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlayer_calls = map(1:n_layers, function(i) {\n    layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n    activation_fn = if (i <= length(activations)) activations[[i]] else NULL\n    \n    result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n    if (!is.null(activation_fn)) {\n        result = append(result, list(call2(activation_fn)))\n    }\n    result\n}) |> \n    unlist() |> \n    compact()\n```\n:::\n\n\nWhat it does is it builds a sequence of operations for the forward pass: layer calls and their activation functions. I stored the output into `layer_calls` so that we can keep track of it. \n\n**The process:**\n\n1.  For each layer, create a list containing:\n\n    - The layer call: `self$fc1()`\n    - The activation call (if exists): `nnf_relu()`\n\n2.  Flatten all lists into a single sequence with `unlist()`. \n\n3.  Filter the list we created away from any `NULL` values with `purrr::compact()`. \n\nThus, we form a list of expressions:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlist(\n    self$fc1(), nnf_relu(),\n    self$fc2(), nnf_relu(),\n    self$fc3(), nnf_relu(),\n    self$fc4(), nnf_relu(),\n    self$fc5(), nnf_relu(),\n    self$out()\n)\n```\n:::\n\n\n**Note:** The last layer (`out`) has no activation because we set it to `NA`.\n\n#### Building an actual body and function\n\nI choose to chain them, `x` or the input as the initial value, and choose not to break lines and forms multiple assignments. This is what I preferred, and besides, it's so easy to form chained expression when the output is a defused call with `reduce()`. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nforward_body = reduce(layer_calls, function(acc, call) {\n    expr(!!acc %>% !!call)\n}, .init = expr(x))\n```\n:::\n\n\nI choose to chain all operations together with pipe operator `%>%` from 'magrittr'.\n\nThen, with `reduce()` works:\n\n1.  Starting with `x`, it progressively adds each operation:\n\n    -   Step 1: x %>% self$fc1()\n    -   Step 2: (x %>% self$fc1()) %>% nnf_relu()\n    -   Step 3: (x %>% self$fc1() %>% nnf_relu()) %>% self$fc2()\n    -   ...and so on\n    \n2.  As for the final output: \n\n    ```r\n    x %>% self$fc1() %>% nnf_relu() %>% \n        self$fc2() %>% nnf_relu() %>% \n        self$fc3() %>% nnf_relu() %>% \n        self$fc4() %>% nnf_relu() %>% \n        self$fc5() %>% nnf_relu() %>% \n        self$out()\n    ```\n\n::: {.callout-tip title=\"Why pipes?\"}\n\nThe pipe operator makes the forward pass logic read like a natural sequence: \"take input x, pass through `fc1`, apply `nnf_relu` to invoke ReLU activation function, then pass through `fc2`, apply `nnf_relu` to invoke ReLU activation function, ..., it kepts repeating until we reach to `out`\"\n\n:::\n\nAfter that, I stored it into `forward_body`, then make use of it to build the function for `forward` method with `rlang::new_function()`: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nforward = new_function(\n    args = list(x = expr()), \n    body = call2(\"{\", forward_body)\n)\n```\n:::\n\n\nThe `args` for `forward` method has `x` with empty value. Then, wrap the piped forward pass into a function that accepts input `x`.\n\nAnd here's the result:\n\n```r\nfunction(x) {\n    x %>% self$fc1() %>% nnf_relu() %>% \n        self$fc2() %>% nnf_relu() %>% \n        self$fc3() %>% nnf_relu() %>% \n        self$fc4() %>% nnf_relu() %>% \n        self$fc5() %>% nnf_relu() %>% \n        self$out()\n}\n```\n\n*Store this expression into `forward` because we still have to finalize the expression we want to create. *\n\n### Step 7: Finalizing the `nn_module` Expression generation\n\nHere we are for the final part: generating the `nn_module` expression, by puzzling each part: `initialize` and `forward`. \n\nThe final part is built from this:\n\n``` r\ncall2(\"nn_module\", nn_name, !!!set_names(list(init, forward), c(\"initialize\", \"forward\")))\n```\n\nI mean, you still have to use `call2()` to build a call. The inputs should be:\n\n1.  `.fn = \"nn_module\"` -> \n2.  The rest of the arguments:\n\n    -  `nn_name` which is equivalent to \"DeepFFN\". You can set any names whatever you want, though.\n    -  `initialize = init`\n    -  `forward = forward`\n    -   Originally, I formed this in this expression: `!!!set_names(list(init, forward), c(\"initialize\", \"forward\"))`. But then, I realized that we only need `initialize` and `forward`, and besides, this is a bit overkill. \n\nThus, the final expression that defines the neural network module.\n\nAnd hence, I form a function that generates a, perhaps, template: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhd_nodes = c(20, 30, 20, 15, 20)\nact_fns = c(\"nnf_relu\", \"nnf_relu\", \"nnf_relu\", \"nnf_relu\")\ncreate_nn_module(\n    hd_neurons = hd_nodes, \n    activations = act_fns\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnn_module(\"DeepFFN\", initialize = function () \n{\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}, forward = function (x) \n{\n    x %>% self$fc1() %>% nnf_relu() %>% self$fc2() %>% nnf_relu() %>% \n        self$fc3() %>% nnf_relu() %>% self$fc4() %>% nnf_relu() %>% \n        self$fc5() %>% self$out()\n})\n```\n\n\n:::\n:::\n\n\n## Disclaimer\n\nThis is an advanced example of metaprogramming in R, demonstrating how to leverage functional programming and `rlang` for code generation. I don't mind you to replicate what I did, but sometimes this technique should be used judiciously—sometimes simpler, more explicit code is better.\n\nThis example showcases:\n\n-   Deep understanding of R's evaluation model\n-   Functional programming with `purrr`\n-   Expression manipulation with `rlang`\n-   Practical application to deep learning workflows\n\nAnd also, I am aware to the fact that the function I made is ugly if you said so. \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}