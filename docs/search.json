[
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Want my help?",
    "section": "",
    "text": "To join is simple: Complete the form by clicking the hyperlinked text\n\n\nCore of R programming: Lectures about the heart of R programming, and it has 3 courses.\nCore of Python programming (not available right now): Lectures about the heart of R programming\nBayesianism (not available right now): Stan probabilistic programming is the main course.\n\n\n\n\n\nEmail me at: ongcoyjoshuamarie@gmail.com if you want to reach me\n\n\nData Manipulation\nData Visualization\nCalculus & Linear Algebra\nStatistical Modelling & Machine Learning\nResearch & Development\n\n\nFurther instruction: Here is the template subject to make a deal: “Consultation: Data Manipulation”"
  },
  {
    "objectID": "services.html#tutorials",
    "href": "services.html#tutorials",
    "title": "Want my help?",
    "section": "",
    "text": "To join is simple: Complete the form by clicking the hyperlinked text\n\n\nCore of R programming: Lectures about the heart of R programming, and it has 3 courses.\nCore of Python programming (not available right now): Lectures about the heart of R programming\nBayesianism (not available right now): Stan probabilistic programming is the main course."
  },
  {
    "objectID": "services.html#consultancy",
    "href": "services.html#consultancy",
    "title": "Want my help?",
    "section": "",
    "text": "Email me at: ongcoyjoshuamarie@gmail.com if you want to reach me\n\n\nData Manipulation\nData Visualization\nCalculus & Linear Algebra\nStatistical Modelling & Machine Learning\nResearch & Development\n\n\nFurther instruction: Here is the template subject to make a deal: “Consultation: Data Manipulation”"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html",
    "href": "posts/02-arima-grid-search/index.html",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "How do you train ARIMA model in your time series models? Grid search, or Hyndman and Khandaka (2008) algorithm? I created this document to demonstrate you how to fit every possible ARIMA models using grid search with visualization. This is a basic problem of hyperparameter tuning. I prepare a simulated time series, and visualize the fitted values of every possible ARIMA models with ‘ggplot2’ and make it interactive with ggiraph.\n\n\n\nThe ARIMA (AutoRegressive Integrated Moving Average) model is defined by three parameters:\n\np: Autoregressive order that counts the past lagged terms (This is AR in ARIMA context)\nd: Differencing order that counts the number of differencing to achieve stationarity (This is ‘I’ or “integrate” in ARIMA context)\nq: Moving average order that counts the past error lagged terms (MA)\n\nChoosing the right combination of (p, d, q) is…not that easy, right when you want to achieve the best fit, even with Hyndman and Khandaka (2008) methodology with their forecast::auto.arima().\nThis is how it’s done:\n\nPrepare a time series data. I generate a time series data from this in order for you to replicate this.\n\nThen fit every possible ARIMA models across a grid of (p, d, q) values.\n\nThen evaluate the models performance by calculating the maximum log-likelihood then weight them with AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\n\nThen visualize the fitted values from every possible models, alongside the actual data.\n\nFrom the visual, highlight the best model in red.\n\nOptionally, you can make it interactive, using ‘ggiraph’, and I prepare it so that you can hover and explore model fits.\n\nThe packages used:\n\nbox (v1.2.0)\nggplot2 (v4.0.0)\nggiraph (v0.9.1)\npurrr (v1.0.2)\ndplyr (v1.1.4)\nforecast (v8.23.0)\nglue (v1.7.0)\ntidyr (v1.3.1)\nrlang (v1.1.4)\nscales (v1.4.0)\n\n\n\n\n\nWe generate a synthetic dataset with some trend and randomness:\n\nset.seed(123)\nts_sim = runif(365, 5, 10) + seq(-140, 224)^2 / 10000\nday = as.Date(\"2025-06-14\") - 0:364\n\nThis produces 365 daily observations with both trend and noise, which is a good test case for ARIMA.\n\n\n\n\nWe test a grid of ARIMA parameters:\n\\[\n\\begin{aligned}\np &\\in \\{0,1,2\\} \\\\\nd &\\in \\{0,1\\} \\\\\nq &\\in \\{0,1,2\\}\n\\end{aligned}\n\\]\nWe exclude overly complex models where p + q &gt; 3.\n\n\nCode\nmodels = local({\n    box::use(\n        purrr[pmap, pmap_chr, possibly, map, map_dbl], \n        dplyr[transmute, mutate, filter, slice_min, slice, case_when], \n        forecast[Arima], \n        glue[glue], \n        tidyr[expand_grid], \n        rlang[exec]\n    )\n    \n    expand_grid(p = 0:2, d = 0:1, q = 0:2) |&gt; \n        transmute(\n            models = pmap_chr(\n                pick(1:3), \n                \\(p, d, q) glue(\"ARIMA({p},{d},{q})\")\n            ), \n            res = pmap(\n                pick(1:3),\n                possibly(\n                    function (p, d, q) {\n                        if (p + q &gt; 3) return(NULL)\n                        exec(Arima, as.ts(ts_sim), order = c(p, d, q))\n                    },\n                    otherwise = NULL\n                )\n            ), \n            fits = map(res, ~ if(is.null(.x)) NULL else fitted(.x)),\n            aic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else AIC(.x)),\n            bic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else BIC(.x))\n        ) |&gt; \n        filter(!is.na(aic)) |&gt;  # Remove failed models\n        mutate(\n            day = list(day),\n            is_lowest_aic = aic == min(aic, na.rm = TRUE),\n            is_lowest_bic = bic == min(bic, na.rm = TRUE)\n        )\n})\nmodels\n\n\n\n  \n\n\n\nThis gives us a nested data frame of fitted models with their AIC, BIC, and fitted values.\n\n\n\nDo you want to visualize everything better, including the actual, fitted values, and highlight the fitted values made by the best fit?\nFrom models data, we just need the is_lowest_aic and is_lowest_bic. We just need to tweak the data a little bit here by expanding the fitted values with its corresponding data value. Then, set the model_type to condition the plotting data with dplyr::case_when().\n\nplot_data = local({\n    box::use(\n        tidyr[unnest], \n        dplyr[mutate, case_when]\n    )\n    models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            )\n        )\n})\n\nOptionally:\n\nYou can put the information about the model which had the lowest AIC and BIC in an annotated box text in the plot.\n\nbest_model_1 = models |&gt; dplyr::filter(is_lowest_aic)\nbest_model_2 = models |&gt; dplyr::filter(is_lowest_bic)\n\nPointing out the maximum value of the time series data\n\nmax_val = max(ts_sim)\nmax_idx = which.max(ts_sim)\nmax_day = day[max_idx]\n\n\nThen, visualize:\n\n\nCode\nlocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter], \n        glue[glue], \n    )\n    \n    p = ggplot() + \n        # Original data\n        geom_line(\n            aes(x = day, y = as.numeric(ts_sim)), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point(\n            aes(x = day, y = as.numeric(ts_sim)), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        # Other fitted models (light gray)\n        geom_line(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        # Best BIC model (blue)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        # Best AIC model (red, on top)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        # Peak annotation\n        annotate(\n            \"point\",\n            x = max_day, y = max_val,\n            size = 4, colour = \"#E74C3C\", shape = 21, stroke = 2, fill = \"white\"\n        ) + \n        annotate(\n            \"text\", \n            x = max_day + 25,\n            y = max_val + 0.5,\n            label = paste0(\"Peak: \", round(max_val, 1), \" sec\"), \n            color = \"#E74C3C\",\n            fontface = \"bold\",\n            size = 3.5\n        ) +\n        annotate(\n            \"curve\", \n            x = max_day + 20, xend = max_day + 0.1, \n            y = max_val + 0.3, yend = max_val, \n            linewidth = 0.8,\n            color = \"#E74C3C\", \n            curvature = -0.2,\n            arrow = arrow(length = unit(0.15, \"cm\"), type = \"closed\")\n        ) +\n        \n        # Model performance text box\n        annotate(\n            \"rect\",\n            xmin = min(day) + 20, xmax = min(day) + 100,\n            ymin = max(ts_sim) - 1.5, ymax = max(ts_sim) - 0.2,\n            fill = \"white\", color = \"#34495E\", alpha = 0.9\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 0.5,\n            label = paste0(\"Best AIC: \", best_model_1$models, \n                           \"\\nAIC = \", round(best_model_1$aic, 1)),\n            color = \"#E74C3C\", fontface = \"bold\", size = 3.2\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 1.1,\n            label = paste0(\"Best BIC: \", best_model_2$models,\n                           \"\\nBIC = \", round(best_model_2$bic, 1)),\n            color = \"#3498DB\", fontface = \"bold\", size = 3.2\n        ) +\n        \n        # Styling\n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    p\n})\n\n\n\n\n\n\n\n\n\nWe visualize:\n\nOriginal data (dark gray)\nAll fitted values from every possible ARIMA model (light gray), except, the fitted values from the best fit is highlighted (red, blue, based on AIC, BIC, respectively)\nAnnotated peak point in the data\nAnnotated best AIC/BIC values\n\n\n\n\n\nIf you prefer your plot to be interactive like some figures in the website, use ‘ggiraph’ interactive interface version of ggplot2, then girafe(). The output produced by girafe() is wrapped with HTML, so it can be run in web.\nI recommend ‘ggiraph’ to build web applications in R.\nThis is the interactive version of the plot above:\n\n\nCode\nlocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter, mutate, case_when], \n        glue[glue], \n        ggiraph[geom_line_interactive, geom_point_interactive, girafe, opts_hover, opts_hover_inv, opts_selection], \n        tidyr[unnest]\n    )\n    \n    # Use this for preparation\n    original_data = data.frame(\n        day = day,\n        ts_sim = ts_sim,\n        tooltip_line = \"actual data\",\n        tooltip_point = paste0(format(day, \"%Y-%m-%d\"), \"; readings: \", round(ts_sim, 1), \" secs\")\n    )\n    \n    plot_data = models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            ),\n            # Create tooltip text for model lines\n            tooltip_text = case_when(\n                is_lowest_aic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                is_lowest_bic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                TRUE ~ paste0(\"model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2))\n            )\n        )\n  \n    p = ggplot() + \n        geom_line_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_line, data_id = \"original_line\"), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_point), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    interactive_plot = girafe(\n        ggobj = p,\n        options = list(\n            opts_hover(css = \"cursor:pointer;stroke-width:4;stroke-opacity:1;fill-opacity:1;r:4px;\"),\n            opts_hover_inv(css = \"opacity:0.1;\"),\n            opts_selection(type = \"none\")\n        )\n    )\n    \n    interactive_plot\n})\n\n\n\n\n\n\n\n\n\n\nThis is just a toy example of leveraging functional programming and basic hyperparameter tuning for time series in R, and some of my learning competencies about data visualization in R and how to get deeper in it.\nIf you are interested to learn more, check out my other gists."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#introduction",
    "href": "posts/02-arima-grid-search/index.html#introduction",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "The ARIMA (AutoRegressive Integrated Moving Average) model is defined by three parameters:\n\np: Autoregressive order that counts the past lagged terms (This is AR in ARIMA context)\nd: Differencing order that counts the number of differencing to achieve stationarity (This is ‘I’ or “integrate” in ARIMA context)\nq: Moving average order that counts the past error lagged terms (MA)\n\nChoosing the right combination of (p, d, q) is…not that easy, right when you want to achieve the best fit, even with Hyndman and Khandaka (2008) methodology with their forecast::auto.arima().\nThis is how it’s done:\n\nPrepare a time series data. I generate a time series data from this in order for you to replicate this.\n\nThen fit every possible ARIMA models across a grid of (p, d, q) values.\n\nThen evaluate the models performance by calculating the maximum log-likelihood then weight them with AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion).\n\nThen visualize the fitted values from every possible models, alongside the actual data.\n\nFrom the visual, highlight the best model in red.\n\nOptionally, you can make it interactive, using ‘ggiraph’, and I prepare it so that you can hover and explore model fits.\n\nThe packages used:\n\nbox (v1.2.0)\nggplot2 (v4.0.0)\nggiraph (v0.9.1)\npurrr (v1.0.2)\ndplyr (v1.1.4)\nforecast (v8.23.0)\nglue (v1.7.0)\ntidyr (v1.3.1)\nrlang (v1.1.4)\nscales (v1.4.0)"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#simulating-data",
    "href": "posts/02-arima-grid-search/index.html#simulating-data",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "We generate a synthetic dataset with some trend and randomness:\n\nset.seed(123)\nts_sim = runif(365, 5, 10) + seq(-140, 224)^2 / 10000\nday = as.Date(\"2025-06-14\") - 0:364\n\nThis produces 365 daily observations with both trend and noise, which is a good test case for ARIMA."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#fitting-multiple-arima-models",
    "href": "posts/02-arima-grid-search/index.html#fitting-multiple-arima-models",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "We test a grid of ARIMA parameters:\n\\[\n\\begin{aligned}\np &\\in \\{0,1,2\\} \\\\\nd &\\in \\{0,1\\} \\\\\nq &\\in \\{0,1,2\\}\n\\end{aligned}\n\\]\nWe exclude overly complex models where p + q &gt; 3.\n\n\nCode\nmodels = local({\n    box::use(\n        purrr[pmap, pmap_chr, possibly, map, map_dbl], \n        dplyr[transmute, mutate, filter, slice_min, slice, case_when], \n        forecast[Arima], \n        glue[glue], \n        tidyr[expand_grid], \n        rlang[exec]\n    )\n    \n    expand_grid(p = 0:2, d = 0:1, q = 0:2) |&gt; \n        transmute(\n            models = pmap_chr(\n                pick(1:3), \n                \\(p, d, q) glue(\"ARIMA({p},{d},{q})\")\n            ), \n            res = pmap(\n                pick(1:3),\n                possibly(\n                    function (p, d, q) {\n                        if (p + q &gt; 3) return(NULL)\n                        exec(Arima, as.ts(ts_sim), order = c(p, d, q))\n                    },\n                    otherwise = NULL\n                )\n            ), \n            fits = map(res, ~ if(is.null(.x)) NULL else fitted(.x)),\n            aic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else AIC(.x)),\n            bic = map_dbl(res, ~ if(is.null(.x)) NA_real_ else BIC(.x))\n        ) |&gt; \n        filter(!is.na(aic)) |&gt;  # Remove failed models\n        mutate(\n            day = list(day),\n            is_lowest_aic = aic == min(aic, na.rm = TRUE),\n            is_lowest_bic = bic == min(bic, na.rm = TRUE)\n        )\n})\nmodels\n\n\n\n  \n\n\n\nThis gives us a nested data frame of fitted models with their AIC, BIC, and fitted values."
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#visualizing-models-with-ggplot2",
    "href": "posts/02-arima-grid-search/index.html#visualizing-models-with-ggplot2",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "Do you want to visualize everything better, including the actual, fitted values, and highlight the fitted values made by the best fit?\nFrom models data, we just need the is_lowest_aic and is_lowest_bic. We just need to tweak the data a little bit here by expanding the fitted values with its corresponding data value. Then, set the model_type to condition the plotting data with dplyr::case_when().\n\nplot_data = local({\n    box::use(\n        tidyr[unnest], \n        dplyr[mutate, case_when]\n    )\n    models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            )\n        )\n})\n\nOptionally:\n\nYou can put the information about the model which had the lowest AIC and BIC in an annotated box text in the plot.\n\nbest_model_1 = models |&gt; dplyr::filter(is_lowest_aic)\nbest_model_2 = models |&gt; dplyr::filter(is_lowest_bic)\n\nPointing out the maximum value of the time series data\n\nmax_val = max(ts_sim)\nmax_idx = which.max(ts_sim)\nmax_day = day[max_idx]\n\n\nThen, visualize:\n\n\nCode\nlocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter], \n        glue[glue], \n    )\n    \n    p = ggplot() + \n        # Original data\n        geom_line(\n            aes(x = day, y = as.numeric(ts_sim)), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point(\n            aes(x = day, y = as.numeric(ts_sim)), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        # Other fitted models (light gray)\n        geom_line(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        # Best BIC model (blue)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        # Best AIC model (red, on top)\n        geom_line(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        # Peak annotation\n        annotate(\n            \"point\",\n            x = max_day, y = max_val,\n            size = 4, colour = \"#E74C3C\", shape = 21, stroke = 2, fill = \"white\"\n        ) + \n        annotate(\n            \"text\", \n            x = max_day + 25,\n            y = max_val + 0.5,\n            label = paste0(\"Peak: \", round(max_val, 1), \" sec\"), \n            color = \"#E74C3C\",\n            fontface = \"bold\",\n            size = 3.5\n        ) +\n        annotate(\n            \"curve\", \n            x = max_day + 20, xend = max_day + 0.1, \n            y = max_val + 0.3, yend = max_val, \n            linewidth = 0.8,\n            color = \"#E74C3C\", \n            curvature = -0.2,\n            arrow = arrow(length = unit(0.15, \"cm\"), type = \"closed\")\n        ) +\n        \n        # Model performance text box\n        annotate(\n            \"rect\",\n            xmin = min(day) + 20, xmax = min(day) + 100,\n            ymin = max(ts_sim) - 1.5, ymax = max(ts_sim) - 0.2,\n            fill = \"white\", color = \"#34495E\", alpha = 0.9\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 0.5,\n            label = paste0(\"Best AIC: \", best_model_1$models, \n                           \"\\nAIC = \", round(best_model_1$aic, 1)),\n            color = \"#E74C3C\", fontface = \"bold\", size = 3.2\n        ) +\n        annotate(\n            \"text\",\n            x = min(day) + 60, y = max(ts_sim) - 1.1,\n            label = paste0(\"Best BIC: \", best_model_2$models,\n                           \"\\nBIC = \", round(best_model_2$bic, 1)),\n            color = \"#3498DB\", fontface = \"bold\", size = 3.2\n        ) +\n        \n        # Styling\n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    p\n})\n\n\n\n\n\n\n\n\n\nWe visualize:\n\nOriginal data (dark gray)\nAll fitted values from every possible ARIMA model (light gray), except, the fitted values from the best fit is highlighted (red, blue, based on AIC, BIC, respectively)\nAnnotated peak point in the data\nAnnotated best AIC/BIC values"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#optional-interactive-visualization-with-ggiraph",
    "href": "posts/02-arima-grid-search/index.html#optional-interactive-visualization-with-ggiraph",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "If you prefer your plot to be interactive like some figures in the website, use ‘ggiraph’ interactive interface version of ggplot2, then girafe(). The output produced by girafe() is wrapped with HTML, so it can be run in web.\nI recommend ‘ggiraph’ to build web applications in R.\nThis is the interactive version of the plot above:\n\n\nCode\nlocal({\n    box::use(\n        ggplot2[...],\n        scales[comma],\n        dplyr[filter, mutate, case_when], \n        glue[glue], \n        ggiraph[geom_line_interactive, geom_point_interactive, girafe, opts_hover, opts_hover_inv, opts_selection], \n        tidyr[unnest]\n    )\n    \n    # Use this for preparation\n    original_data = data.frame(\n        day = day,\n        ts_sim = ts_sim,\n        tooltip_line = \"actual data\",\n        tooltip_point = paste0(format(day, \"%Y-%m-%d\"), \"; readings: \", round(ts_sim, 1), \" secs\")\n    )\n    \n    plot_data = models |&gt; \n        unnest(cols = c(day, fits)) |&gt;\n        mutate(\n            model_type = case_when(\n                is_lowest_aic ~ \"Best AIC\",\n                is_lowest_bic ~ \"Best BIC\", \n                TRUE ~ \"Other Models\"\n            ),\n            # Create tooltip text for model lines\n            tooltip_text = case_when(\n                is_lowest_aic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                is_lowest_bic ~ paste0(\"best model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2)),\n                TRUE ~ paste0(\"model: \", models, \"; AIC = \", round(aic, 2), \"; BIC = \", round(bic, 2))\n            )\n        )\n  \n    p = ggplot() + \n        geom_line_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_line, data_id = \"original_line\"), \n            linewidth = 1.2, \n            color = \"#2C3E50\",\n            alpha = 0.8\n        ) + \n        geom_point_interactive(\n            data = original_data,\n            aes(x = day, y = ts_sim, tooltip = tooltip_point), \n            size = 0.8, \n            color = \"#2C3E50\", \n            alpha = 0.6\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Other Models\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#BDC3C7',\n            alpha = 0.6,\n            linewidth = 0.5\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best BIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#3498DB',\n            linewidth = 1.2,\n            alpha = 0.9\n        ) + \n        \n        geom_line_interactive(\n            data = filter(plot_data, model_type == \"Best AIC\"), \n            aes(x = day, y = fits, group = models, tooltip = tooltip_text, data_id = models), \n            color = '#E74C3C',\n            linewidth = 1.5,\n            alpha = 0.9\n        ) + \n        \n        scale_x_date(date_labels = \"%b %Y\", date_breaks = \"2 months\") +\n        scale_y_continuous(labels = comma) +\n        \n        labs(\n            x = \"Time Index\", \n            y = \"Simulated Response Values\", \n            title = \"ARIMA Model Grid Search: Simulated Time Series Analysis\",\n            subtitle = glue(\"Comparing {nrow(models)} successful ARIMA models • Best performing models highlighted\"),\n            caption = paste0(\"Models tested: p∈[0,2], d∈[0,1], q∈[0,2] • \",\n                             \"Original data shown in dark gray • \",\n                             \"Total observations: \", length(ts_sim))\n        ) + \n        \n        theme_minimal(base_size = 11, base_family = \"serif\") +\n        theme(\n            plot.title = element_text(\n                family = \"serif\", \n                colour = \"#2C3E50\",\n                size = 14,\n                face = \"bold\",\n                margin = margin(b = 5)\n            ),\n            plot.subtitle = element_text(\n                family = \"serif\",\n                colour = \"#7F8C8D\",\n                size = 10,\n                margin = margin(b = 15)\n            ),\n            plot.caption = element_text(\n                family = \"serif\",\n                colour = \"#95A5A6\",\n                size = 8,\n                margin = margin(t = 10)\n            ),\n            axis.text.x = element_text(\n                angle = 45, \n                hjust = 1,\n                margin = margin(t = 8),\n                color = \"#34495E\"\n            ), \n            axis.text.y = element_text(\n                margin = margin(r = 8),\n                color = \"#34495E\"\n            ), \n            axis.title = element_text(\n                color = \"#2C3E50\",\n                face = \"bold\"\n            ),\n            panel.grid.minor = element_blank(),\n            panel.grid.major = element_line(\n                color = \"#ECF0F1\", \n                linewidth = 0.3\n            ), \n            panel.background = element_rect(fill = \"#FEFEFE\", color = NA),\n            plot.background = element_rect(fill = \"white\", color = NA),\n            plot.margin = margin(20, 20, 20, 20)\n        ) \n    \n    interactive_plot = girafe(\n        ggobj = p,\n        options = list(\n            opts_hover(css = \"cursor:pointer;stroke-width:4;stroke-opacity:1;fill-opacity:1;r:4px;\"),\n            opts_hover_inv(css = \"opacity:0.1;\"),\n            opts_selection(type = \"none\")\n        )\n    )\n    \n    interactive_plot\n})"
  },
  {
    "objectID": "posts/02-arima-grid-search/index.html#disclaimer",
    "href": "posts/02-arima-grid-search/index.html#disclaimer",
    "title": "First level of time series modelling: Basic ARIMA model hyperparameter tuning",
    "section": "",
    "text": "This is just a toy example of leveraging functional programming and basic hyperparameter tuning for time series in R, and some of my learning competencies about data visualization in R and how to get deeper in it.\nIf you are interested to learn more, check out my other gists."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My name is Joshua",
    "section": "",
    "text": "Computer Scientist • Statistician\n\n\nI am a (self-proclaimed) computer scientist, technical person, and a statistician. I studied layers of programming paradigms. From what I studied so far, R and Python have closely equivalent feature parities. I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields.\n\nKnow About Me"
  },
  {
    "objectID": "index.html#joshua-marie-ongcoy",
    "href": "index.html#joshua-marie-ongcoy",
    "title": "My name is Joshua",
    "section": "",
    "text": "Computer Scientist • Statistician\n\n\nI am a (self-proclaimed) computer scientist, technical person, and a statistician. I studied layers of programming paradigms. From what I studied so far, R and Python have closely equivalent feature parities. I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields.\n\nKnow About Me"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "My name is Joshua",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nFirst level of time series modelling: Basic ARIMA model hyperparameter tuning\n\n\nWith plots\n\n\n\nR\n\n\ntime-series\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\nSep 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression\n\n\nUse case of code generation in practice\n\n\n\nR\n\n\nmachine-learning\n\n\ntorch\n\n\npointless-code\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "About Me - Card Layout\n    \n    \n    \n    \n    \n    \n    \n    \n\n    \n\n\n\n\n\n  \n   \n    \n      I'm a \n      \n        \n          Computer Scientist\n          Statistician\n          Data Scientist\n          Consultant\nHowdy! I’m your neighborhood statistics man — a proud statistics man with some passions for mathematics, statistics, and making sense of the chaos. I serve as a data whisperer, tutor, and a consultant. I am a (self-proclaimed) computer scientist, technical person, and a statistician. I studied layers of programming paradigms. From what I studied so far, R and Python have closely equivalent feature parities. I write packages for statistics, data science, and numerical analysis, as my specialties found in those fields."
  },
  {
    "objectID": "about.html#formal-education",
    "href": "about.html#formal-education",
    "title": "About Me",
    "section": "Formal Education",
    "text": "Formal Education"
  },
  {
    "objectID": "about.html#skillsets",
    "href": "about.html#skillsets",
    "title": "About Me",
    "section": "Skillsets",
    "text": "Skillsets"
  },
  {
    "objectID": "about.html#employment",
    "href": "about.html#employment",
    "title": "About Me",
    "section": "Employment",
    "text": "Employment\n\n\nStatistician @ Fixstat\nPackage maintainer"
  },
  {
    "objectID": "posts/01-meta-nn/index.html",
    "href": "posts/01-meta-nn/index.html",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "I have a course tutorial, which I discuss the things to get “advance” in R. Code generation is part of it. My blog compiles pointless codes in pointless code series, and this is one of them.\nNow, the question is: How do you define neural network architectures in your deep learning projects? Manually write out each layer? Copy-paste and modify existing code? In this part, I wanted to discuss it to you on how to leverage code generation technique that generates ‘torch’ neural network modules in a programmatic approach. This is a handy approach to building flexible, reusable neural network architectures without repetitive code.\nI’ll walk through a function that creates DFFNN expressions with customizable architectures, layer by layer, explaining each step along the way.\n\n\n\nThe create_nn_module() function dynamically generates torch neural network module definitions. Instead of manually writing out layer definitions and forward pass logic, this function builds the code expressions for you.\nKey benefits:\n\nFlexibility: Change network architecture with a single function call\nAutomation: Generate multiple network configurations programmatically\nExperimentation: Quickly test different architectures in hyperparameter searches\n\nThis is how it’s done:\n\nDefine the network architecture (input size, hidden layers, output size)\nSpecify activation functions for each layer\nProgrammatically generate the initialize method (layer definitions)\nProgrammatically generate the forward method (forward pass logic)\nReturn an nn_module expression ready to be evaluated\n\nThe packages used:\n\nrlang (v1.1.4) - For metaprogramming tools\npurrr (v1.0.2) - For functional programming\nglue (v1.7.0) - For string interpolation\nmagrittr - For pipe operator\nbox (v1.2.0) - For modular code organization\n\n\n\n\nI created create_nn_module() function a while ago and shared it on GitHub Gist. Here’s the function we’ll be analyzing:\n\n\nCode\ncreate_nn_module = function(nn_name = \"DeepFFN\", hd_neurons = c(20, 30, 20, 15), no_x = 10, no_y = 1, activations = NULL) {\n    box::use(\n        rlang[new_function, call2, caller_env, expr, exprs, sym, is_function, env_get_list],\n        purrr[map, map2, reduce, set_names, compact, map_if, keep, map_lgl], \n        glue[glue], \n        magrittr[`%&gt;%`]\n    )\n    \n    nodes = c(no_x, hd_neurons, no_y)\n    n_layers = length(nodes) - 1\n    \n    call_args = match.call()\n    activation_arg = call_args$activations\n    \n    if (is.null(activations)) {\n        activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n    } else if (length(activations) == 1 || is.function(activations)) {\n        single_activation = activations\n        activations = c(rep(list(single_activation), length(hd_neurons)), list(NA))\n    }\n    \n    activations = map2(activations, seq_along(activations), function(x, i) {\n        if (is.null(x)) {\n            NULL\n        } else if (is.function(x)) {\n            if(!is.null(activation_arg) && is.call(activation_arg) && activation_arg[[1]] == quote(c)) {\n                func_name = as.character(activation_arg[[i + 1]])\n                sym(func_name)\n            } else if(!is.null(activation_arg) && (is.symbol(activation_arg) || is.character(activation_arg))) {\n                func_name = as.character(activation_arg)\n                sym(func_name)\n            } else {\n                parent_env = parent.frame()\n                env_names = ls(envir = parent_env)\n                matching_names = env_names %&gt;%\n                    keep(~ {\n                        obj = env_get_list(parent_env, .x)[[1]]\n                        identical(obj, x)\n                    })\n                \n                if (length(matching_names) &gt; 0) {\n                    sym(matching_names[1])\n                } else {\n                    stop(\"Could not determine function name for activation function\")\n                }\n            }\n        } else if (is.character(x)) {\n            if (length(x) == 1 && is.na(x)) {\n                NULL\n            } else {\n                sym(x)\n            }\n        } else if (is.symbol(x)) {\n            x\n        } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n        }\n    })\n    \n    init_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), call2(\"nn_linear\", !!!dims))\n    })\n    \n    init = new_function(\n        args = list(), \n        body = call2(\"{\", !!!init_body)\n    )\n    \n    layer_calls = map(1:n_layers, function(i) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n        \n        result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n        if (!is.null(activation_fn)) {\n            result = append(result, list(call2(activation_fn)))\n        }\n        result\n    }) |&gt; \n        unlist() |&gt; # recursive = FALSE is also valid\n        compact()\n    \n    forward_body = reduce(layer_calls, function(acc, call) {\n        expr(!!acc %&gt;% !!call)\n    }, .init = expr(x))\n    \n    forward = new_function(\n        args = list(x = expr()), \n        body = call2(\"{\", forward_body)\n    )\n    \n    call2(\"nn_module\", nn_name, initialize = init, forward = forward)\n}\n\n\n\n\n\nYou’ll notice that I’ve been using another approach to load namespace in R. But, why ‘box’? You need to check out my mini book dedicated on modular programming in R.\n\n\n\nWell, a function, or a function call, creates an environment, which encloses the objects and operations within it. In other words, we create a closure. This is actually a good practice for several reasons:\n\nNamespace isolation: Dependencies loaded inside the function will not make pollution the global environment, or conflicts with any packages loaded. When you load packages required with library(), inside a function or not, it attaches those packages to your search path, and will mask functions from other packages. With box::use() inside a function, the imports are scoped only to that function’s or call’s environment.\nExplicit dependencies: Anyone reading the function immediately knows what external tools it uses. You don’t have to scroll to the top of a script to see what’s loaded.\nReproducibility: The function becomes self-contained. If you share just this function, others know exactly what packages they need less hunting through documentation.\nAvoiding conflicts: Different functions can use different versions or implementations without conflicts. For example, one function might use dplyr::filter() while another uses stats::filter(), and they won’t interfere with each other.\nLazy loading: The packages are only loaded when the function is actually called, not when it’s defined. This can improve script startup time if you have many functions but only use a few.\n\n\n\n\n\n\n\nNote\n\n\n\nIn a nutshell: The ‘box’ package provides explicit, granular imports, making it transparent which namespace to be imported from which packages. It’s like having a well-organized toolbox where each tool is labeled.\n\n\n\n\n\n\n\nI’ll be trying to be concise on explaining each layers of the function so that you’ll understand what I did\n\n\nI use box::use() to load dependencies:\n\nrlang: Improvised Core R programming. One of the core R programming, metaprogramming which includes creating expressions and functions programmatically, are less painful than what base R have.\npurrr: Improvised Functional programming utilities.\nglue: R lacks Python’s f-string for string interpolation, although we have sprintf() and paste() for that. glue makes string interpolation more readable with glue(\"fc{i}\") instead of paste0(\"fc\", i) or sprintf(\"fc%d\", i).\nmagrittr: The pipe operator %&gt;% for chaining operations. This is optional, by the way — R 4.1+ has the native pipe |&gt;, but %&gt;% offers better flexibility with the dot placeholder.\n\n\n\n\nIn DFFNN architecture, it is defined by the input layer, the hidden layer, and the output layer.\n\n\n\nSource: https://medium.com/data-science/designing-your-neural-networks-a5e4617027ed\n\n\nThe number of nodes are defined by integers, except for input and output layer nodes which they are fixed and determined by the data you provided, and they are defined by no_x and no_y. The number of hidden layers is defined by the length of input in hd_neurons argument.\nCombine no_x, hd_neurons, no_y in order:\nnodes = c(no_x, hd_neurons, no_y)\nAnd then calculate the length of nodes, which is \\(1 + n_{\\text{hidden layres}} + 1\\), and then subtract it by 1 because the applied activation functions is invoked between each layer.\nn_layers = length(nodes) - 1\n\n\nWhen you have:\n\n10 predictors\n5 hidden layers, and for each layer:\n\n20 nodes\n30 nodes\n20 nodes\n15 nodes\n20 nodes\n\n1 response variable\n\nTotal number of layers: 7\nThis means we need 7 - 1 linear transformations, and here is my diagram:\n\\[10_{\\text{inputs}} \\rightarrow20_{\\text{hd1}} \\rightarrow30_{\\text{hd2}} \\rightarrow20_{\\text{hd3}} \\rightarrow15_{\\text{hd4}} \\rightarrow20_{\\text{hd5}} \\rightarrow1_{\\text{ouput}}\\]\n\n\n\n\nThe activations argument holds the account of the activation function. It could be a string, a literal function, or a mix of it in a vector of inputs.\nThen, set activations = NULL, where NULL is the default value, which leads to set ReLU (nnf_relu) as the activation function for all hidden layers\n\n\nCode\nif (is.null(activations)) {\n    activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n}\n\n\nEvery activations will have NA as the last element because we need to ensure no activation function after the output. The output layer often doesn’t need an activation (for regression) or needs a specific one based on the task (softmax for classification, sigmoid for binary classification). By defaulting to NA, the user can decide.\n\n\n\n\n\n\nLength of inputs\n\n\n\nTo provide values in activations argument, it needs to be equal to the size of hidden layers, or if you provide only 1 act. function, this will be the activation function across the transformations.\n\n\n\n\n\n\n\n\nDefault\n\n\n\nThe default is NULL. That is, if activations is not provided, the activation function is set to ReLU function.\n\n\n\n\n\n\n\n\nInstead of NULL\n\n\n\nNow, if you’re asking “Why needs to set activations to \"nnf_relu\" instead of NULL”? Don’t worry, I did consider that, but this is just a pure demo.\n\n\n\n\n\nThis part (re)processes the activation function inputs in the activations argument. This kept tracks the argument you are putting, especially when you the input you are writing in activations argument has different types.\n\n\nCode\ncall_args = match.call()\nactivation_arg = call_args$activations\n\nactivations = map2(activations, seq_along(activations), function(x, i) {\n    if (is.null(x)) {\n        NULL\n    } else if (is.function(x)) {\n        if(!is.null(activation_arg) && is.call(activation_arg) && \n           activation_arg[[1]] == quote(c)) {\n            func_name = as.character(activation_arg[[i + 1]])\n            sym(func_name)\n        } else {\n            func_name = names(which(sapply(ls(envir = parent.frame()), \n                function(name) {\n                    identical(get(name, envir = parent.frame()), x)\n                })))[1]\n            if (!is.na(func_name)) {\n                sym(func_name)\n            } else {\n                stop(\"Could not determine function name for activation function\")\n            }\n        }\n    } else if (is.character(x)) {\n        if (length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            sym(x)\n        }\n    } else if (is.symbol(x)) {\n        x\n    } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n        NULL\n    } else {\n        stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n    }\n})\n\n\n\n\n\nThe body I am referring in initialize method is the body of the function for the initialize implemented method. This part is a bit far from trivial. I named it init_body to keep track the expression I am trying to build.\n\n\n\n\n\n\nIn reality\n\n\n\nKeep in mind that there’s no initialize and forward parameters within nn_module() torch namespace or whatsoever. However, it is expected you to create them to create a module inside nn_module(). These parameters are kept within the ... wildcard parameter.\n\n\n\n\nHere is the part I am tracking inside create_nn_module body expression:\n\n\nCode\ninit_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), \n    function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), \n              call2(\"nn_linear\", !!!dims))\n    })\n\n\nWhat it does is it creates assignment expressions for each layer in the network.\nFor instance, c(20, 30, 20, 15, 20) is your argument for the activations:\n\nmap2(nodes[-length(nodes)], nodes[-1], c) pairs consecutive layer sizes:\n\n\nCode\nlist(\n    c(10, 20), \n    c(20, 30), \n    c(30, 20), \n    c(20, 15), \n    c(15, 20), \n    c(20, 1)\n)\n\n\nFor each pair, generates a layer assignment expression:\n\nLayer names: fc1, fc2, …, out (last layer)\nCreates: self$fc1 = nn_linear(10, 20)\n\n\nThis will be the generated expression:\nself$fc1 = nn_linear(10, 20)\nself$fc2 = nn_linear(20, 30)\nself$fc3 = nn_linear(30, 20)\nself$fc4 = nn_linear(20, 15)\nself$fc5 = nn_linear(15, 20)\nself$out = nn_linear(20, 1)\n\n\n\n\n\n\nHow is it done?\n\n\n\nI need you to understand rlang::call2() a bit:\nThe call2() function is a glorified call() from base R that builds function call expressions.\nFrom what I did within init_body:\n\ncall2(\"$\", expr(self), sym(\"fc1\")) constructs self$fc1\ncall2(\"nn_linear\", !!!dims) is a bit complex:\n\nIt splices dims from what I created in map2(nodes[-length(nodes)], nodes[-1], c).\ncall2() function accepts rlang’s quasiquotation API, then splices the dimensions, i.e. call2(\"nn_linear\", !!!c(10, 20)) to call2(\"nn_linear\", 10, 20).\nThen finally constructs nn_linear(10, 20)\n\ncall2(\"=\", lhs, rhs) parses an expression: lhs = rhs. This part yields an expression I want: self$fc1 = nn_linear(10, 20).\n\nNote: You can use &lt;- if you want, instead of =. After all, = within call2()’s .fn argument tokenize = as an assignment operator. \n\n\n\n\n\nNow, for this part:\n\n\nCode\ninit = new_function(\n    args = list(), \n    body = call2(\"{\", !!!init_body)\n)\n\n\nDon’t forget to put curly brackets { around the built expression because it becomes necessary in R when composing a function with multiple lines. Still using call2() for that, particularly call2(\"{\", !!!init_body) creates a code block { ... } containing all initialization statements. The !!! operator “splices” the list of expressions into the block, because init_body forms a list of expressions.\nAfter building the expression I want for the body of initialize, let’s take further by utilizing it as a body to create a function with rlang::new_function. I just simply wraps all the layer initialization expressions into a complete function for initialize method for nn_module().\n\n\n\n\n\n\nInputs in initialize\n\n\n\nNotice that the argument for initialize is empty? I could’ve place input_size and output_size if I wanted to, but it seems unnecessary since I already placed the sizes of the input and output within the expression I built. To make a function expression with empty arguments, place the args argument of new_function with empty list().\n\n\nHere’s the result:\nfunction() {\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}\nStore this expression into init because we still have to finalize the expression we want to create. \n\n\n\n\nThe same process as initialize, except we are not building multiple lines of expression, just building a chained expression with ‘magrittr’ pipe from the initial value.\n\n\nTo form this expression is also complex\n\n\nCode\nlayer_calls = map(1:n_layers, function(i) {\n    layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n    activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n    \n    result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n    if (!is.null(activation_fn)) {\n        result = append(result, list(call2(activation_fn)))\n    }\n    result\n}) |&gt; \n    unlist() |&gt; \n    compact()\n\n\nWhat it does is it builds a sequence of operations for the forward pass: layer calls and their activation functions. I stored the output into layer_calls so that we can keep track of it.\nThe process:\n\nFor each layer, create a list containing:\n\nThe layer call: self$fc1()\nThe activation call (if exists): nnf_relu()\n\nFlatten all lists into a single sequence with unlist().\nFilter the list we created away from any NULL values with purrr::compact().\n\nThus, we form a list of expressions:\n\n\nCode\nlist(\n    self$fc1(), nnf_relu(),\n    self$fc2(), nnf_relu(),\n    self$fc3(), nnf_relu(),\n    self$fc4(), nnf_relu(),\n    self$fc5(), nnf_relu(),\n    self$out()\n)\n\n\nNote: The last layer (out) has no activation because we set it to NA.\n\n\n\nI choose to chain them, x or the input as the initial value, and choose not to break lines and forms multiple assignments. This is what I preferred, and besides, it’s so easy to form chained expression when the output is a defused call with reduce().\n\n\nCode\nforward_body = reduce(layer_calls, function(acc, call) {\n    expr(!!acc %&gt;% !!call)\n}, .init = expr(x))\n\n\nI choose to chain all operations together with pipe operator %&gt;% from ‘magrittr’.\nThen, with reduce() works:\n\nStarting with x, it progressively adds each operation:\n\nStep 1: x %&gt;% self$fc1()\nStep 2: (x %&gt;% self$fc1()) %&gt;% nnf_relu()\nStep 3: (x %&gt;% self$fc1() %&gt;% nnf_relu()) %&gt;% self$fc2()\n…and so on\n\nAs for the final output:\nx %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n    self$fc2() %&gt;% nnf_relu() %&gt;% \n    self$fc3() %&gt;% nnf_relu() %&gt;% \n    self$fc4() %&gt;% nnf_relu() %&gt;% \n    self$fc5() %&gt;% nnf_relu() %&gt;% \n    self$out()\n\n\n\n\n\n\n\nWhy pipes?\n\n\n\nThe pipe operator makes the forward pass logic read like a natural sequence: “take input x, pass through fc1, apply nnf_relu to invoke ReLU activation function, then pass through fc2, apply nnf_relu to invoke ReLU activation function, …, it kepts repeating until we reach to out”\n\n\nAfter that, I stored it into forward_body, then make use of it to build the function for forward method with rlang::new_function():\n\n\nCode\nforward = new_function(\n    args = list(x = expr()), \n    body = call2(\"{\", forward_body)\n)\n\n\nThe args for forward method has x with empty value. Then, wrap the piped forward pass into a function that accepts input x.\nAnd here’s the result:\nfunction(x) {\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n        self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% \n        self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% nnf_relu() %&gt;% \n        self$out()\n}\nStore this expression into forward because we still have to finalize the expression we want to create. \n\n\n\n\nHere we are for the final part: generating the nn_module expression, by puzzling each part: initialize and forward.\nThe final part is built from this:\ncall2(\"nn_module\", nn_name, !!!set_names(list(init, forward), c(\"initialize\", \"forward\")))\nI mean, you still have to use call2() to build a call. The inputs should be:\n\n.fn = \"nn_module\" -&gt;\nThe rest of the arguments:\n\nnn_name which is equivalent to “DeepFFN”. You can set any names whatever you want, though.\ninitialize = init\nforward = forward\nOriginally, I formed this in this expression: !!!set_names(list(init, forward), c(\"initialize\", \"forward\")). But then, I realized that we only need initialize and forward, and besides, this is a bit overkill.\n\n\nThus, the final expression that defines the neural network module.\nAnd hence, I form a function that generates a, perhaps, template:\n\nhd_nodes = c(20, 30, 20, 15, 20)\nact_fns = c(\"nnf_relu\", \"nnf_relu\", \"nnf_relu\", \"nnf_relu\")\ncreate_nn_module(\n    hd_neurons = hd_nodes, \n    activations = act_fns\n)\n\nnn_module(\"DeepFFN\", initialize = function () \n{\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}, forward = function (x) \n{\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% self$out()\n})\n\n\n\n\n\n\nThis is an advanced example of metaprogramming in R, demonstrating how to leverage functional programming and rlang for code generation. I don’t mind you to replicate what I did, but sometimes this technique should be used judiciously—sometimes simpler, more explicit code is better.\nThis example showcases:\n\nDeep understanding of R’s evaluation model\nFunctional programming with purrr\nExpression manipulation with rlang\nPractical application to deep learning workflows\n\nAnd also, I am aware to the fact that the function I made is ugly if you said so."
  },
  {
    "objectID": "posts/01-meta-nn/index.html#introduction",
    "href": "posts/01-meta-nn/index.html#introduction",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "The create_nn_module() function dynamically generates torch neural network module definitions. Instead of manually writing out layer definitions and forward pass logic, this function builds the code expressions for you.\nKey benefits:\n\nFlexibility: Change network architecture with a single function call\nAutomation: Generate multiple network configurations programmatically\nExperimentation: Quickly test different architectures in hyperparameter searches\n\nThis is how it’s done:\n\nDefine the network architecture (input size, hidden layers, output size)\nSpecify activation functions for each layer\nProgrammatically generate the initialize method (layer definitions)\nProgrammatically generate the forward method (forward pass logic)\nReturn an nn_module expression ready to be evaluated\n\nThe packages used:\n\nrlang (v1.1.4) - For metaprogramming tools\npurrr (v1.0.2) - For functional programming\nglue (v1.7.0) - For string interpolation\nmagrittr - For pipe operator\nbox (v1.2.0) - For modular code organization"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#the-complete-function",
    "href": "posts/01-meta-nn/index.html#the-complete-function",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "I created create_nn_module() function a while ago and shared it on GitHub Gist. Here’s the function we’ll be analyzing:\n\n\nCode\ncreate_nn_module = function(nn_name = \"DeepFFN\", hd_neurons = c(20, 30, 20, 15), no_x = 10, no_y = 1, activations = NULL) {\n    box::use(\n        rlang[new_function, call2, caller_env, expr, exprs, sym, is_function, env_get_list],\n        purrr[map, map2, reduce, set_names, compact, map_if, keep, map_lgl], \n        glue[glue], \n        magrittr[`%&gt;%`]\n    )\n    \n    nodes = c(no_x, hd_neurons, no_y)\n    n_layers = length(nodes) - 1\n    \n    call_args = match.call()\n    activation_arg = call_args$activations\n    \n    if (is.null(activations)) {\n        activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n    } else if (length(activations) == 1 || is.function(activations)) {\n        single_activation = activations\n        activations = c(rep(list(single_activation), length(hd_neurons)), list(NA))\n    }\n    \n    activations = map2(activations, seq_along(activations), function(x, i) {\n        if (is.null(x)) {\n            NULL\n        } else if (is.function(x)) {\n            if(!is.null(activation_arg) && is.call(activation_arg) && activation_arg[[1]] == quote(c)) {\n                func_name = as.character(activation_arg[[i + 1]])\n                sym(func_name)\n            } else if(!is.null(activation_arg) && (is.symbol(activation_arg) || is.character(activation_arg))) {\n                func_name = as.character(activation_arg)\n                sym(func_name)\n            } else {\n                parent_env = parent.frame()\n                env_names = ls(envir = parent_env)\n                matching_names = env_names %&gt;%\n                    keep(~ {\n                        obj = env_get_list(parent_env, .x)[[1]]\n                        identical(obj, x)\n                    })\n                \n                if (length(matching_names) &gt; 0) {\n                    sym(matching_names[1])\n                } else {\n                    stop(\"Could not determine function name for activation function\")\n                }\n            }\n        } else if (is.character(x)) {\n            if (length(x) == 1 && is.na(x)) {\n                NULL\n            } else {\n                sym(x)\n            }\n        } else if (is.symbol(x)) {\n            x\n        } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n        }\n    })\n    \n    init_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), call2(\"nn_linear\", !!!dims))\n    })\n    \n    init = new_function(\n        args = list(), \n        body = call2(\"{\", !!!init_body)\n    )\n    \n    layer_calls = map(1:n_layers, function(i) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n        \n        result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n        if (!is.null(activation_fn)) {\n            result = append(result, list(call2(activation_fn)))\n        }\n        result\n    }) |&gt; \n        unlist() |&gt; # recursive = FALSE is also valid\n        compact()\n    \n    forward_body = reduce(layer_calls, function(acc, call) {\n        expr(!!acc %&gt;% !!call)\n    }, .init = expr(x))\n    \n    forward = new_function(\n        args = list(x = expr()), \n        body = call2(\"{\", forward_body)\n    )\n    \n    call2(\"nn_module\", nn_name, initialize = init, forward = forward)\n}\n\n\n\n\n\nYou’ll notice that I’ve been using another approach to load namespace in R. But, why ‘box’? You need to check out my mini book dedicated on modular programming in R.\n\n\n\nWell, a function, or a function call, creates an environment, which encloses the objects and operations within it. In other words, we create a closure. This is actually a good practice for several reasons:\n\nNamespace isolation: Dependencies loaded inside the function will not make pollution the global environment, or conflicts with any packages loaded. When you load packages required with library(), inside a function or not, it attaches those packages to your search path, and will mask functions from other packages. With box::use() inside a function, the imports are scoped only to that function’s or call’s environment.\nExplicit dependencies: Anyone reading the function immediately knows what external tools it uses. You don’t have to scroll to the top of a script to see what’s loaded.\nReproducibility: The function becomes self-contained. If you share just this function, others know exactly what packages they need less hunting through documentation.\nAvoiding conflicts: Different functions can use different versions or implementations without conflicts. For example, one function might use dplyr::filter() while another uses stats::filter(), and they won’t interfere with each other.\nLazy loading: The packages are only loaded when the function is actually called, not when it’s defined. This can improve script startup time if you have many functions but only use a few.\n\n\n\n\n\n\n\nNote\n\n\n\nIn a nutshell: The ‘box’ package provides explicit, granular imports, making it transparent which namespace to be imported from which packages. It’s like having a well-organized toolbox where each tool is labeled."
  },
  {
    "objectID": "posts/01-meta-nn/index.html#explanations",
    "href": "posts/01-meta-nn/index.html#explanations",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "I’ll be trying to be concise on explaining each layers of the function so that you’ll understand what I did\n\n\nI use box::use() to load dependencies:\n\nrlang: Improvised Core R programming. One of the core R programming, metaprogramming which includes creating expressions and functions programmatically, are less painful than what base R have.\npurrr: Improvised Functional programming utilities.\nglue: R lacks Python’s f-string for string interpolation, although we have sprintf() and paste() for that. glue makes string interpolation more readable with glue(\"fc{i}\") instead of paste0(\"fc\", i) or sprintf(\"fc%d\", i).\nmagrittr: The pipe operator %&gt;% for chaining operations. This is optional, by the way — R 4.1+ has the native pipe |&gt;, but %&gt;% offers better flexibility with the dot placeholder.\n\n\n\n\nIn DFFNN architecture, it is defined by the input layer, the hidden layer, and the output layer.\n\n\n\nSource: https://medium.com/data-science/designing-your-neural-networks-a5e4617027ed\n\n\nThe number of nodes are defined by integers, except for input and output layer nodes which they are fixed and determined by the data you provided, and they are defined by no_x and no_y. The number of hidden layers is defined by the length of input in hd_neurons argument.\nCombine no_x, hd_neurons, no_y in order:\nnodes = c(no_x, hd_neurons, no_y)\nAnd then calculate the length of nodes, which is \\(1 + n_{\\text{hidden layres}} + 1\\), and then subtract it by 1 because the applied activation functions is invoked between each layer.\nn_layers = length(nodes) - 1\n\n\nWhen you have:\n\n10 predictors\n5 hidden layers, and for each layer:\n\n20 nodes\n30 nodes\n20 nodes\n15 nodes\n20 nodes\n\n1 response variable\n\nTotal number of layers: 7\nThis means we need 7 - 1 linear transformations, and here is my diagram:\n\\[10_{\\text{inputs}} \\rightarrow20_{\\text{hd1}} \\rightarrow30_{\\text{hd2}} \\rightarrow20_{\\text{hd3}} \\rightarrow15_{\\text{hd4}} \\rightarrow20_{\\text{hd5}} \\rightarrow1_{\\text{ouput}}\\]\n\n\n\n\nThe activations argument holds the account of the activation function. It could be a string, a literal function, or a mix of it in a vector of inputs.\nThen, set activations = NULL, where NULL is the default value, which leads to set ReLU (nnf_relu) as the activation function for all hidden layers\n\n\nCode\nif (is.null(activations)) {\n    activations = c(rep(\"nnf_relu\", length(hd_neurons)), NA)\n}\n\n\nEvery activations will have NA as the last element because we need to ensure no activation function after the output. The output layer often doesn’t need an activation (for regression) or needs a specific one based on the task (softmax for classification, sigmoid for binary classification). By defaulting to NA, the user can decide.\n\n\n\n\n\n\nLength of inputs\n\n\n\nTo provide values in activations argument, it needs to be equal to the size of hidden layers, or if you provide only 1 act. function, this will be the activation function across the transformations.\n\n\n\n\n\n\n\n\nDefault\n\n\n\nThe default is NULL. That is, if activations is not provided, the activation function is set to ReLU function.\n\n\n\n\n\n\n\n\nInstead of NULL\n\n\n\nNow, if you’re asking “Why needs to set activations to \"nnf_relu\" instead of NULL”? Don’t worry, I did consider that, but this is just a pure demo.\n\n\n\n\n\nThis part (re)processes the activation function inputs in the activations argument. This kept tracks the argument you are putting, especially when you the input you are writing in activations argument has different types.\n\n\nCode\ncall_args = match.call()\nactivation_arg = call_args$activations\n\nactivations = map2(activations, seq_along(activations), function(x, i) {\n    if (is.null(x)) {\n        NULL\n    } else if (is.function(x)) {\n        if(!is.null(activation_arg) && is.call(activation_arg) && \n           activation_arg[[1]] == quote(c)) {\n            func_name = as.character(activation_arg[[i + 1]])\n            sym(func_name)\n        } else {\n            func_name = names(which(sapply(ls(envir = parent.frame()), \n                function(name) {\n                    identical(get(name, envir = parent.frame()), x)\n                })))[1]\n            if (!is.na(func_name)) {\n                sym(func_name)\n            } else {\n                stop(\"Could not determine function name for activation function\")\n            }\n        }\n    } else if (is.character(x)) {\n        if (length(x) == 1 && is.na(x)) {\n            NULL\n        } else {\n            sym(x)\n        }\n    } else if (is.symbol(x)) {\n        x\n    } else if (is.logical(x) && length(x) == 1 && is.na(x)) {\n        NULL\n    } else {\n        stop(\"Activation must be a function, string, symbol, NA, or NULL\")\n    }\n})\n\n\n\n\n\nThe body I am referring in initialize method is the body of the function for the initialize implemented method. This part is a bit far from trivial. I named it init_body to keep track the expression I am trying to build.\n\n\n\n\n\n\nIn reality\n\n\n\nKeep in mind that there’s no initialize and forward parameters within nn_module() torch namespace or whatsoever. However, it is expected you to create them to create a module inside nn_module(). These parameters are kept within the ... wildcard parameter.\n\n\n\n\nHere is the part I am tracking inside create_nn_module body expression:\n\n\nCode\ninit_body = map2(1:n_layers, map2(nodes[-length(nodes)], nodes[-1], c), \n    function(i, dims) {\n        layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n        call2(\"=\", call2(\"$\", expr(self), sym(layer_name)), \n              call2(\"nn_linear\", !!!dims))\n    })\n\n\nWhat it does is it creates assignment expressions for each layer in the network.\nFor instance, c(20, 30, 20, 15, 20) is your argument for the activations:\n\nmap2(nodes[-length(nodes)], nodes[-1], c) pairs consecutive layer sizes:\n\n\nCode\nlist(\n    c(10, 20), \n    c(20, 30), \n    c(30, 20), \n    c(20, 15), \n    c(15, 20), \n    c(20, 1)\n)\n\n\nFor each pair, generates a layer assignment expression:\n\nLayer names: fc1, fc2, …, out (last layer)\nCreates: self$fc1 = nn_linear(10, 20)\n\n\nThis will be the generated expression:\nself$fc1 = nn_linear(10, 20)\nself$fc2 = nn_linear(20, 30)\nself$fc3 = nn_linear(30, 20)\nself$fc4 = nn_linear(20, 15)\nself$fc5 = nn_linear(15, 20)\nself$out = nn_linear(20, 1)\n\n\n\n\n\n\nHow is it done?\n\n\n\nI need you to understand rlang::call2() a bit:\nThe call2() function is a glorified call() from base R that builds function call expressions.\nFrom what I did within init_body:\n\ncall2(\"$\", expr(self), sym(\"fc1\")) constructs self$fc1\ncall2(\"nn_linear\", !!!dims) is a bit complex:\n\nIt splices dims from what I created in map2(nodes[-length(nodes)], nodes[-1], c).\ncall2() function accepts rlang’s quasiquotation API, then splices the dimensions, i.e. call2(\"nn_linear\", !!!c(10, 20)) to call2(\"nn_linear\", 10, 20).\nThen finally constructs nn_linear(10, 20)\n\ncall2(\"=\", lhs, rhs) parses an expression: lhs = rhs. This part yields an expression I want: self$fc1 = nn_linear(10, 20).\n\nNote: You can use &lt;- if you want, instead of =. After all, = within call2()’s .fn argument tokenize = as an assignment operator. \n\n\n\n\n\nNow, for this part:\n\n\nCode\ninit = new_function(\n    args = list(), \n    body = call2(\"{\", !!!init_body)\n)\n\n\nDon’t forget to put curly brackets { around the built expression because it becomes necessary in R when composing a function with multiple lines. Still using call2() for that, particularly call2(\"{\", !!!init_body) creates a code block { ... } containing all initialization statements. The !!! operator “splices” the list of expressions into the block, because init_body forms a list of expressions.\nAfter building the expression I want for the body of initialize, let’s take further by utilizing it as a body to create a function with rlang::new_function. I just simply wraps all the layer initialization expressions into a complete function for initialize method for nn_module().\n\n\n\n\n\n\nInputs in initialize\n\n\n\nNotice that the argument for initialize is empty? I could’ve place input_size and output_size if I wanted to, but it seems unnecessary since I already placed the sizes of the input and output within the expression I built. To make a function expression with empty arguments, place the args argument of new_function with empty list().\n\n\nHere’s the result:\nfunction() {\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}\nStore this expression into init because we still have to finalize the expression we want to create. \n\n\n\n\nThe same process as initialize, except we are not building multiple lines of expression, just building a chained expression with ‘magrittr’ pipe from the initial value.\n\n\nTo form this expression is also complex\n\n\nCode\nlayer_calls = map(1:n_layers, function(i) {\n    layer_name = if (i == n_layers) \"out\" else glue(\"fc{i}\")\n    activation_fn = if (i &lt;= length(activations)) activations[[i]] else NULL\n    \n    result = list(call2(call2(\"$\", expr(self), sym(layer_name))))\n    if (!is.null(activation_fn)) {\n        result = append(result, list(call2(activation_fn)))\n    }\n    result\n}) |&gt; \n    unlist() |&gt; \n    compact()\n\n\nWhat it does is it builds a sequence of operations for the forward pass: layer calls and their activation functions. I stored the output into layer_calls so that we can keep track of it.\nThe process:\n\nFor each layer, create a list containing:\n\nThe layer call: self$fc1()\nThe activation call (if exists): nnf_relu()\n\nFlatten all lists into a single sequence with unlist().\nFilter the list we created away from any NULL values with purrr::compact().\n\nThus, we form a list of expressions:\n\n\nCode\nlist(\n    self$fc1(), nnf_relu(),\n    self$fc2(), nnf_relu(),\n    self$fc3(), nnf_relu(),\n    self$fc4(), nnf_relu(),\n    self$fc5(), nnf_relu(),\n    self$out()\n)\n\n\nNote: The last layer (out) has no activation because we set it to NA.\n\n\n\nI choose to chain them, x or the input as the initial value, and choose not to break lines and forms multiple assignments. This is what I preferred, and besides, it’s so easy to form chained expression when the output is a defused call with reduce().\n\n\nCode\nforward_body = reduce(layer_calls, function(acc, call) {\n    expr(!!acc %&gt;% !!call)\n}, .init = expr(x))\n\n\nI choose to chain all operations together with pipe operator %&gt;% from ‘magrittr’.\nThen, with reduce() works:\n\nStarting with x, it progressively adds each operation:\n\nStep 1: x %&gt;% self$fc1()\nStep 2: (x %&gt;% self$fc1()) %&gt;% nnf_relu()\nStep 3: (x %&gt;% self$fc1() %&gt;% nnf_relu()) %&gt;% self$fc2()\n…and so on\n\nAs for the final output:\nx %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n    self$fc2() %&gt;% nnf_relu() %&gt;% \n    self$fc3() %&gt;% nnf_relu() %&gt;% \n    self$fc4() %&gt;% nnf_relu() %&gt;% \n    self$fc5() %&gt;% nnf_relu() %&gt;% \n    self$out()\n\n\n\n\n\n\n\nWhy pipes?\n\n\n\nThe pipe operator makes the forward pass logic read like a natural sequence: “take input x, pass through fc1, apply nnf_relu to invoke ReLU activation function, then pass through fc2, apply nnf_relu to invoke ReLU activation function, …, it kepts repeating until we reach to out”\n\n\nAfter that, I stored it into forward_body, then make use of it to build the function for forward method with rlang::new_function():\n\n\nCode\nforward = new_function(\n    args = list(x = expr()), \n    body = call2(\"{\", forward_body)\n)\n\n\nThe args for forward method has x with empty value. Then, wrap the piped forward pass into a function that accepts input x.\nAnd here’s the result:\nfunction(x) {\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% \n        self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% \n        self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% nnf_relu() %&gt;% \n        self$out()\n}\nStore this expression into forward because we still have to finalize the expression we want to create. \n\n\n\n\nHere we are for the final part: generating the nn_module expression, by puzzling each part: initialize and forward.\nThe final part is built from this:\ncall2(\"nn_module\", nn_name, !!!set_names(list(init, forward), c(\"initialize\", \"forward\")))\nI mean, you still have to use call2() to build a call. The inputs should be:\n\n.fn = \"nn_module\" -&gt;\nThe rest of the arguments:\n\nnn_name which is equivalent to “DeepFFN”. You can set any names whatever you want, though.\ninitialize = init\nforward = forward\nOriginally, I formed this in this expression: !!!set_names(list(init, forward), c(\"initialize\", \"forward\")). But then, I realized that we only need initialize and forward, and besides, this is a bit overkill.\n\n\nThus, the final expression that defines the neural network module.\nAnd hence, I form a function that generates a, perhaps, template:\n\nhd_nodes = c(20, 30, 20, 15, 20)\nact_fns = c(\"nnf_relu\", \"nnf_relu\", \"nnf_relu\", \"nnf_relu\")\ncreate_nn_module(\n    hd_neurons = hd_nodes, \n    activations = act_fns\n)\n\nnn_module(\"DeepFFN\", initialize = function () \n{\n    self$fc1 = nn_linear(10, 20)\n    self$fc2 = nn_linear(20, 30)\n    self$fc3 = nn_linear(30, 20)\n    self$fc4 = nn_linear(20, 15)\n    self$fc5 = nn_linear(15, 20)\n    self$out = nn_linear(20, 1)\n}, forward = function (x) \n{\n    x %&gt;% self$fc1() %&gt;% nnf_relu() %&gt;% self$fc2() %&gt;% nnf_relu() %&gt;% \n        self$fc3() %&gt;% nnf_relu() %&gt;% self$fc4() %&gt;% nnf_relu() %&gt;% \n        self$fc5() %&gt;% self$out()\n})"
  },
  {
    "objectID": "posts/01-meta-nn/index.html#disclaimer",
    "href": "posts/01-meta-nn/index.html#disclaimer",
    "title": "Automatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression",
    "section": "",
    "text": "This is an advanced example of metaprogramming in R, demonstrating how to leverage functional programming and rlang for code generation. I don’t mind you to replicate what I did, but sometimes this technique should be used judiciously—sometimes simpler, more explicit code is better.\nThis example showcases:\n\nDeep understanding of R’s evaluation model\nFunctional programming with purrr\nExpression manipulation with rlang\nPractical application to deep learning workflows\n\nAnd also, I am aware to the fact that the function I made is ugly if you said so."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Check out my blogs",
    "section": "",
    "text": "First level of time series modelling: Basic ARIMA model hyperparameter tuning\n\n\nWith plots\n\n\n\nR\n\n\ntime-series\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\nSep 27, 2025\n\n\nJoshua Marie Ongcoy\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatically generate Deep Feedforward Neural Network (DFFNN) module from torch expression\n\n\nUse case of code generation in practice\n\n\n\nR\n\n\nmachine-learning\n\n\ntorch\n\n\npointless-code\n\n\n\n\n\n\n\n\n\nSep 23, 2025\n\n\nJoshua Marie Ongcoy\n\n\n\n\n\n\nNo matching items"
  }
]